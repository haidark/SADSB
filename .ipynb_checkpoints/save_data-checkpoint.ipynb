{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import dicom\n",
    "from scipy.misc import imresize\n",
    "from utils import preprocess, rotation_augmentation, shift_augmentation\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "img_resize = True\n",
    "img_shape = (256, 256)\n",
    "DATA_DIR = '/media/haidar/Storage/Data/SADSB/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_resize(img):\n",
    "    \"\"\"\n",
    "    Crop center and resize.\n",
    "\n",
    "    :param img: image to be cropped and resized.\n",
    "    \"\"\"\n",
    "    if img.shape[0] < img.shape[1]:\n",
    "        img = img.T\n",
    "    # we crop image from center\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    img = crop_img\n",
    "    img = imresize(img, img_shape)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_images(from_dir, verbose=True):\n",
    "    \"\"\"\n",
    "    Load images in the form study x slices x width x height.\n",
    "    Each image contains 30 time series frames so that it is ready for the convolutional network.\n",
    "\n",
    "    :param from_dir: directory with images (train or validate)\n",
    "    :param verbose: if true then print data\n",
    "    \"\"\"\n",
    "    print('-'*50)\n",
    "    print('Loading all DICOM images from {0}...'.format(from_dir))\n",
    "    print('-'*50)\n",
    "\n",
    "    current_study_sub = ''  # saves the current study sub_folder\n",
    "    current_study = ''  # saves the current study folder\n",
    "    current_study_images = []  # holds current study images\n",
    "    ids = []  # keeps the ids of the studies\n",
    "    study_to_images = dict()  # dictionary for studies to images\n",
    "    total = 0\n",
    "    images = []  # saves 30-frame-images\n",
    "    from_dir = from_dir if from_dir.endswith('/') else from_dir + '/'\n",
    "    for subdir, _, files in os.walk(from_dir):\n",
    "        subdir = subdir.replace('\\\\', '/')  # windows path fix\n",
    "        subdir_split = subdir.split('/')\n",
    "        study_id = subdir_split[-3]\n",
    "        if \"sax\" in subdir:\n",
    "            for f in files:\n",
    "                image_path = os.path.join(subdir, f)\n",
    "                if not image_path.endswith('.dcm'):\n",
    "                    continue\n",
    "\n",
    "                image = dicom.read_file(image_path)\n",
    "                image = image.pixel_array.astype(float)\n",
    "                image /= np.max(image)  # scale to [0,1]\n",
    "                if img_resize:\n",
    "                    image = crop_resize(image)\n",
    "\n",
    "                if current_study_sub != subdir:\n",
    "                    x = 0\n",
    "                    try:\n",
    "                        while len(images) < 30:\n",
    "                            images.append(images[x])\n",
    "                            x += 1\n",
    "                        if len(images) > 30:\n",
    "                            images = images[0:30]\n",
    "\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    current_study_sub = subdir\n",
    "                    current_study_images.append(images)\n",
    "                    images = []\n",
    "\n",
    "                if current_study != study_id:\n",
    "                    study_to_images[current_study] = np.array(current_study_images)\n",
    "                    if current_study != \"\":\n",
    "                        ids.append(current_study)\n",
    "                    current_study = study_id\n",
    "                    current_study_images = []\n",
    "                images.append(image)\n",
    "                if verbose:\n",
    "                    if total % 1000 == 0:\n",
    "                        print('Images processed {0}'.format(total))\n",
    "                total += 1\n",
    "    x = 0\n",
    "    try:\n",
    "        while len(images) < 30:\n",
    "            images.append(images[x])\n",
    "            x += 1\n",
    "        if len(images) > 30:\n",
    "            images = images[0:30]\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    print('-'*50)\n",
    "    print('All DICOM in {0} images loaded.'.format(from_dir))\n",
    "    print('-'*50)\n",
    "\n",
    "    current_study_images.append(images)\n",
    "    study_to_images[current_study] = np.array(current_study_images)\n",
    "    if current_study != \"\":\n",
    "        ids.append(current_study)\n",
    "\n",
    "    return ids, study_to_images\n",
    "\n",
    "\n",
    "def map_studies_results():\n",
    "    \"\"\"\n",
    "    Maps studies to their respective targets.\n",
    "    \"\"\"\n",
    "    id_to_results = dict()\n",
    "    train_csv = open(DATA_DIR+'train.csv')\n",
    "    lines = train_csv.readlines()\n",
    "    i = 0\n",
    "    for item in lines:\n",
    "        if i == 0:\n",
    "            i = 1\n",
    "            continue\n",
    "        id, diastole, systole = item.replace('\\n', '').split(',')\n",
    "        id_to_results[id] = [float(diastole), float(systole)]\n",
    "\n",
    "    return id_to_results\n",
    "\n",
    "def append_data(ds, data):\n",
    "    curRows = ds.shape[0]\n",
    "    newRows = data.shape[0]\n",
    "    ds.resize(curRows+newRows, axis=0)\n",
    "    ds[curRows:, ...] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Writing training data to .h5 file...\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Loading all DICOM images from /media/haidar/Storage/Data/SADSB/train...\n",
      "--------------------------------------------------\n",
      "Images processed 0\n",
      "Images processed 1000\n",
      "Images processed 2000\n",
      "Images processed 3000\n",
      "Images processed 4000\n",
      "Images processed 5000\n",
      "Images processed 6000\n",
      "Images processed 7000\n",
      "Images processed 8000\n",
      "Images processed 9000\n",
      "Images processed 10000\n",
      "Images processed 11000\n",
      "Images processed 12000\n",
      "Images processed 13000\n",
      "Images processed 14000\n",
      "Images processed 15000\n",
      "Images processed 16000\n",
      "Images processed 17000\n",
      "Images processed 18000\n",
      "Images processed 19000\n",
      "Images processed 20000\n",
      "Images processed 21000\n",
      "Images processed 22000\n",
      "Images processed 23000\n",
      "Images processed 24000\n",
      "Images processed 25000\n",
      "Images processed 26000\n",
      "Images processed 27000\n",
      "Images processed 28000\n",
      "Images processed 29000\n",
      "Images processed 30000\n",
      "Images processed 31000\n",
      "Images processed 32000\n",
      "Images processed 33000\n",
      "Images processed 34000\n",
      "Images processed 35000\n",
      "Images processed 36000\n",
      "Images processed 37000\n",
      "Images processed 38000\n",
      "Images processed 39000\n",
      "Images processed 40000\n",
      "Images processed 41000\n",
      "Images processed 42000\n",
      "Images processed 43000\n",
      "Images processed 44000\n",
      "Images processed 45000\n",
      "Images processed 46000\n",
      "Images processed 47000\n",
      "Images processed 48000\n",
      "Images processed 49000\n",
      "Images processed 50000\n",
      "Images processed 51000\n",
      "Images processed 52000\n",
      "Images processed 53000\n",
      "Images processed 54000\n",
      "Images processed 55000\n",
      "Images processed 56000\n",
      "Images processed 57000\n",
      "Images processed 58000\n",
      "Images processed 59000\n",
      "Images processed 60000\n",
      "Images processed 61000\n",
      "Images processed 62000\n",
      "Images processed 63000\n",
      "Images processed 64000\n",
      "Images processed 65000\n",
      "Images processed 66000\n",
      "Images processed 67000\n",
      "Images processed 68000\n",
      "Images processed 69000\n",
      "Images processed 70000\n",
      "Images processed 71000\n",
      "Images processed 72000\n",
      "Images processed 73000\n",
      "Images processed 74000\n",
      "Images processed 75000\n",
      "Images processed 76000\n",
      "Images processed 77000\n",
      "Images processed 78000\n",
      "Images processed 79000\n",
      "Images processed 80000\n",
      "Images processed 81000\n",
      "Images processed 82000\n",
      "Images processed 83000\n",
      "Images processed 84000\n",
      "Images processed 85000\n",
      "Images processed 86000\n",
      "Images processed 87000\n",
      "Images processed 88000\n",
      "Images processed 89000\n",
      "Images processed 90000\n",
      "Images processed 91000\n",
      "Images processed 92000\n",
      "Images processed 93000\n",
      "Images processed 94000\n",
      "Images processed 95000\n",
      "Images processed 96000\n",
      "Images processed 97000\n",
      "Images processed 98000\n",
      "Images processed 99000\n",
      "Images processed 100000\n",
      "Images processed 101000\n",
      "Images processed 102000\n",
      "Images processed 103000\n",
      "Images processed 104000\n",
      "Images processed 105000\n",
      "Images processed 106000\n",
      "Images processed 107000\n",
      "Images processed 108000\n",
      "Images processed 109000\n",
      "Images processed 110000\n",
      "Images processed 111000\n",
      "Images processed 112000\n",
      "Images processed 113000\n",
      "Images processed 114000\n",
      "Images processed 115000\n",
      "Images processed 116000\n",
      "Images processed 117000\n",
      "Images processed 118000\n",
      "Images processed 119000\n",
      "Images processed 120000\n",
      "Images processed 121000\n",
      "Images processed 122000\n",
      "Images processed 123000\n",
      "Images processed 124000\n",
      "Images processed 125000\n",
      "Images processed 126000\n",
      "Images processed 127000\n",
      "Images processed 128000\n",
      "Images processed 129000\n",
      "Images processed 130000\n",
      "Images processed 131000\n",
      "Images processed 132000\n",
      "Images processed 133000\n",
      "Images processed 134000\n",
      "Images processed 135000\n",
      "Images processed 136000\n",
      "Images processed 137000\n",
      "Images processed 138000\n",
      "Images processed 139000\n",
      "Images processed 140000\n",
      "Images processed 141000\n",
      "Images processed 142000\n",
      "Images processed 143000\n",
      "Images processed 144000\n",
      "Images processed 145000\n",
      "Images processed 146000\n",
      "Images processed 147000\n",
      "Images processed 148000\n",
      "Images processed 149000\n",
      "Images processed 150000\n",
      "Images processed 151000\n",
      "Images processed 152000\n",
      "Images processed 153000\n",
      "Images processed 154000\n",
      "Images processed 155000\n",
      "Images processed 156000\n",
      "Images processed 157000\n",
      "Images processed 158000\n",
      "Images processed 159000\n",
      "Images processed 160000\n",
      "Images processed 161000\n",
      "--------------------------------------------------\n",
      "All DICOM in /media/haidar/Storage/Data/SADSB/train/ images loaded.\n",
      "--------------------------------------------------\n",
      "Done saving (split) Training data.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loads the training data set including X and y and saves it to .h5 file.\n",
    "\"\"\"\n",
    "print('-'*50)\n",
    "print('Writing training data to .h5 file...')\n",
    "print('-'*50)\n",
    "\n",
    "study_ids, images = load_images(DATA_DIR+'train')  # load images and their ids\n",
    "studies_to_results = map_studies_results()  # load the dictionary of studies to targets\n",
    "\n",
    "study_id = study_ids[0]\n",
    "# x = shift_augmentation(rotation_augmentation(preprocess(images[study_id].astype(np.float32)/255), 15), 0.1, 0.1)\n",
    "x = images[study_id]\n",
    "y = studies_to_results[study_id]\n",
    "\n",
    "YTr = []\n",
    "for j in range(x.shape[0]):\n",
    "    YTr.append(y)\n",
    "    \n",
    "YVal = []\n",
    "for j in range(x.shape[0]):\n",
    "    YVal.append(y)\n",
    "\n",
    "shpX = x.shape[1:]\n",
    "\n",
    "with h5py.File(DATA_DIR+'trainData.h5', 'w') as dataFile:\n",
    "    XTrain = dataFile.create_dataset('XTrain', data=x, \n",
    "                                        maxshape=(None, shpX[0], shpX[1], shpX[2]))\n",
    "    XVal = dataFile.create_dataset('XVal', data=x, \n",
    "                                        maxshape=(None, shpX[0], shpX[1], shpX[2]))\n",
    "    for i in range(1, len(study_ids)):\n",
    "        study_id = study_ids[i]\n",
    "#         x = shift_augmentation(rotation_augmentation(preprocess(images[study_id].astype(np.float32)/255), 15), 0.1, 0.1)\n",
    "        x = images[study_id]\n",
    "        y = studies_to_results[study_id]\n",
    "#         randomly assign to either validation or training (prob of getting into training data = 0.92)\n",
    "        r = np.random.rand()\n",
    "        if r <= 0.92:\n",
    "            append_data(XTrain, x)\n",
    "            for j in range(x.shape[0]):            \n",
    "                YTr.append(y)\n",
    "        else:\n",
    "            append_data(XVal, x)\n",
    "            for j in range(x.shape[0]):            \n",
    "                YVal.append(y)\n",
    "\n",
    "    YTrain = dataFile.create_dataset('YTrain', data=YTr)\n",
    "    YVal = dataFile.create_dataset('YVal', data=YVal)\n",
    "  \n",
    "print('Done saving (split) Training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Writing validation data to .h5 file...\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Loading all DICOM images from /media/haidar/Storage/Data/SADSB/validate...\n",
      "--------------------------------------------------\n",
      "Images processed 0\n",
      "Images processed 1000\n",
      "Images processed 2000\n",
      "Images processed 3000\n",
      "Images processed 4000\n",
      "Images processed 5000\n",
      "Images processed 6000\n",
      "Images processed 7000\n",
      "Images processed 8000\n",
      "Images processed 9000\n",
      "Images processed 10000\n",
      "Images processed 11000\n",
      "Images processed 12000\n",
      "Images processed 13000\n",
      "Images processed 14000\n",
      "Images processed 15000\n",
      "Images processed 16000\n",
      "Images processed 17000\n",
      "Images processed 18000\n",
      "Images processed 19000\n",
      "Images processed 20000\n",
      "Images processed 21000\n",
      "Images processed 22000\n",
      "Images processed 23000\n",
      "Images processed 24000\n",
      "Images processed 25000\n",
      "Images processed 26000\n",
      "Images processed 27000\n",
      "Images processed 28000\n",
      "Images processed 29000\n",
      "Images processed 30000\n",
      "Images processed 31000\n",
      "Images processed 32000\n",
      "Images processed 33000\n",
      "Images processed 34000\n",
      "Images processed 35000\n",
      "Images processed 36000\n",
      "Images processed 37000\n",
      "Images processed 38000\n",
      "Images processed 39000\n",
      "Images processed 40000\n",
      "Images processed 41000\n",
      "Images processed 42000\n",
      "Images processed 43000\n",
      "Images processed 44000\n",
      "Images processed 45000\n",
      "Images processed 46000\n",
      "Images processed 47000\n",
      "Images processed 48000\n",
      "Images processed 49000\n",
      "Images processed 50000\n",
      "Images processed 51000\n",
      "Images processed 52000\n",
      "Images processed 53000\n",
      "Images processed 54000\n",
      "Images processed 55000\n",
      "Images processed 56000\n",
      "Images processed 57000\n",
      "Images processed 58000\n",
      "Images processed 59000\n",
      "Images processed 60000\n",
      "Images processed 61000\n",
      "Images processed 62000\n",
      "Images processed 63000\n",
      "Images processed 64000\n",
      "Images processed 65000\n",
      "--------------------------------------------------\n",
      "All DICOM in /media/haidar/Storage/Data/SADSB/validate/ images loaded.\n",
      "--------------------------------------------------\n",
      "Done saving validation data.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loads the validation data set including X and study ids and saves it to .h5 file.\n",
    "\"\"\"\n",
    "print('-'*50)\n",
    "print('Writing validation data to .h5 file...')\n",
    "print('-'*50)\n",
    "\n",
    "ids, images = load_images(DATA_DIR+'validate')\n",
    "\n",
    "study_id = ids[0]\n",
    "# x = shift_augmentation(rotation_augmentation(preprocess(images[study_id].astype(np.float32)/255), 15), 0.1, 0.1)\n",
    "x = images[study_id]\n",
    "y = study_id\n",
    "Y = []\n",
    "for j in range(x.shape[0]):\n",
    "    Y.append(y)\n",
    "\n",
    "shpX = x.shape[1:]\n",
    "\n",
    "with h5py.File(DATA_DIR+'valData.h5', 'w') as dataFile:\n",
    "    XVal = dataFile.create_dataset('XVal', data=x, \n",
    "                                        maxshape=(None, shpX[0], shpX[1], shpX[2]))\n",
    "        \n",
    "    for i in range(1, len(ids)):\n",
    "        study_id = ids[i]\n",
    "#         x = shift_augmentation(rotation_augmentation(preprocess(images[study_id].astype(np.float32)/255), 15), 0.1, 0.1)\n",
    "        x = images[study_id]\n",
    "        y = study_id\n",
    "        append_data(XVal, x)\n",
    "        for j in range(x.shape[0]):            \n",
    "            Y.append(y)\n",
    "    YVal = dataFile.create_dataset('YVal', data=Y)\n",
    "\n",
    "print('Done saving validation data.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
